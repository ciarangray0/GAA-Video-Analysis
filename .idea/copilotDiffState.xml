<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/interactive_analytics_system_backend/.dockerignore">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/interactive_analytics_system_backend/.dockerignore" />
              <option name="updatedContent" value="# Files to exclude from Docker build context&#10;__pycache__/&#10;*.py[cod]&#10;*$py.class&#10;*.so&#10;&#10;# Virtual environments&#10;venv/&#10;.venv/&#10;ENV/&#10;&#10;# IDE&#10;.idea/&#10;.vscode/&#10;*.swp&#10;&#10;# OS&#10;.DS_Store&#10;&#10;# Testing&#10;.pytest_cache/&#10;tests/&#10;.coverage&#10;htmlcov/&#10;&#10;# Git&#10;.git/&#10;.gitignore&#10;&#10;# Documentation (not needed in container)&#10;*.md&#10;!README.md&#10;&#10;# Development files&#10;.env&#10;.env.local&#10;requirements-dev.txt&#10;&#10;# Data directories (mount as volumes instead)&#10;data/&#10;&#10;# Streamlit (separate deployment)&#10;streamlit_app.py&#10;&#10;# Static frontend (served separately)&#10;static/&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/interactive_analytics_system_backend/.env.example">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/interactive_analytics_system_backend/.env.example" />
              <option name="updatedContent" value="# Environment Configuration Template&#10;# Copy this file to .env and fill in your values&#10;&#10;# Server Configuration&#10;PORT=8000&#10;HOST=0.0.0.0&#10;&#10;# CORS - comma-separated list of allowed origins&#10;# Use specific origins in production, e.g.: https://your-frontend.vercel.app&#10;ALLOWED_ORIGINS=*&#10;&#10;# File Upload Limits&#10;MAX_VIDEO_SIZE_MB=500&#10;&#10;# Data Storage Directory&#10;DATA_DIR=data&#10;&#10;# Model Configuration&#10;YOLO_MODEL_PATH=models/best.pt&#10;&#10;# Logging Level (DEBUG, INFO, WARNING, ERROR)&#10;LOG_LEVEL=INFO&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/interactive_analytics_system_backend/.gitignore">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/interactive_analytics_system_backend/.gitignore" />
              <option name="updatedContent" value="# Python&#10;__pycache__/&#10;*.py[cod]&#10;*$py.class&#10;*.so&#10;.Python&#10;build/&#10;develop-eggs/&#10;dist/&#10;downloads/&#10;eggs/&#10;.eggs/&#10;lib/&#10;lib64/&#10;parts/&#10;sdist/&#10;var/&#10;wheels/&#10;*.egg-info/&#10;.installed.cfg&#10;*.egg&#10;&#10;# Virtual environments&#10;venv/&#10;.venv/&#10;ENV/&#10;env/&#10;&#10;# IDE&#10;.idea/&#10;.vscode/&#10;*.swp&#10;*.swo&#10;*~&#10;&#10;# OS&#10;.DS_Store&#10;.DS_Store?&#10;._*&#10;.Spotlight-V100&#10;.Trashes&#10;ehthumbs.db&#10;Thumbs.db&#10;&#10;# Testing&#10;.pytest_cache/&#10;.coverage&#10;htmlcov/&#10;.tox/&#10;.nox/&#10;&#10;# Data directories (created at runtime)&#10;data/videos/&#10;data/tracks/&#10;data/annotations/&#10;&#10;# Logs&#10;*.log&#10;logs/&#10;&#10;# Environment files (use .env.example for templates)&#10;.env&#10;.env.local&#10;.env.*.local&#10;&#10;# Model files (download separately or mount as volume)&#10;# Uncomment if you want to exclude model files from git:&#10;# models/*.pt&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/interactive_analytics_system_backend/Dockerfile">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/interactive_analytics_system_backend/Dockerfile" />
              <option name="updatedContent" value="# ===== Build Stage =====&#10;FROM python:3.11-slim as builder&#10;&#10;WORKDIR /app&#10;&#10;# Install build dependencies&#10;RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \&#10;    build-essential \&#10;    &amp;&amp; rm -rf /var/lib/apt/lists/*&#10;&#10;# Copy requirements first for better caching&#10;COPY requirements.txt .&#10;&#10;# Install Python dependencies&#10;RUN pip install --no-cache-dir --user -r requirements.txt&#10;&#10;&#10;# ===== Production Stage =====&#10;FROM python:3.11-slim&#10;&#10;WORKDIR /app&#10;&#10;# Install runtime dependencies for OpenCV&#10;RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \&#10;    libgl1-mesa-glx \&#10;    libglib2.0-0 \&#10;    libsm6 \&#10;    libxext6 \&#10;    libxrender-dev \&#10;    libgomp1 \&#10;    ffmpeg \&#10;    &amp;&amp; rm -rf /var/lib/apt/lists/* \&#10;    &amp;&amp; apt-get clean&#10;&#10;# Copy Python packages from builder&#10;COPY --from=builder /root/.local /root/.local&#10;ENV PATH=/root/.local/bin:$PATH&#10;&#10;# Copy application code&#10;COPY app.py main.py ./&#10;COPY pipeline/ ./pipeline/&#10;COPY models/ ./models/&#10;&#10;# Create data directories&#10;RUN mkdir -p data/videos data/tracks data/annotations&#10;&#10;# Environment variables with defaults&#10;ENV PORT=8000 \&#10;    HOST=0.0.0.0 \&#10;    ALLOWED_ORIGINS=* \&#10;    MAX_VIDEO_SIZE_MB=500 \&#10;    DATA_DIR=data \&#10;    YOLO_MODEL_PATH=models/best.pt \&#10;    PYTHONUNBUFFERED=1&#10;&#10;# Expose port&#10;EXPOSE ${PORT}&#10;&#10;# Health check&#10;HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \&#10;    CMD python -c &quot;import urllib.request; urllib.request.urlopen('http://localhost:${PORT}/health')&quot; || exit 1&#10;&#10;# Run the application&#10;CMD [&quot;sh&quot;, &quot;-c&quot;, &quot;uvicorn app:app --host ${HOST} --port ${PORT}&quot;]&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/interactive_analytics_system_backend/README.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/interactive_analytics_system_backend/README.md" />
              <option name="updatedContent" value="# GAA Video Analysis API&#10;&#10;Production-ready FastAPI backend for GAA video analysis with player tracking and trajectory interpolation.&#10;&#10;## Features&#10;&#10;- **Video Upload &amp; Processing** - Upload MP4 videos and extract metadata&#10;- **Player Detection** - YOLOv8 + ByteTrack for player tracking&#10;- **Pitch Calibration** - Homography-based coordinate mapping&#10;- **Trajectory Interpolation** - Linear interpolation between anchor frames&#10;- **RESTful API** - Clean JSON API with OpenAPI documentation&#10;&#10;## Quick Start&#10;&#10;### Local Development&#10;&#10;```bash&#10;# Clone and navigate to backend&#10;cd interactive_analytics_system_backend&#10;&#10;# Create virtual environment&#10;python3 -m venv .venv&#10;source .venv/bin/activate&#10;&#10;# Install dependencies&#10;pip install -r requirements.txt&#10;pip install -r requirements-dev.txt  # For testing&#10;&#10;# Run server&#10;uvicorn app:app --reload --port 8000&#10;```&#10;&#10;### Docker&#10;&#10;```bash&#10;# Build image&#10;docker build -t gaa-video-api .&#10;&#10;# Run container&#10;docker run -p 8000:8000 \&#10;  -e ALLOWED_ORIGINS=https://your-frontend.com \&#10;  -v $(pwd)/data:/app/data \&#10;  gaa-video-api&#10;```&#10;&#10;### Deploy to Render&#10;&#10;1. Connect your GitHub repository to Render&#10;2. Create a new Web Service pointing to `interactive_analytics_system_backend/`&#10;3. Render will auto-detect `render.yaml` configuration&#10;4. Set `ALLOWED_ORIGINS` in the Render dashboard to your frontend URL&#10;&#10;## API Endpoints&#10;&#10;| Method | Endpoint | Description |&#10;|--------|----------|-------------|&#10;| GET | `/health` | Health check |&#10;| POST | `/videos` | Upload video |&#10;| POST | `/videos/{id}/track` | Run player tracking |&#10;| POST | `/videos/{id}/homographies` | Compute homographies |&#10;| POST | `/videos/{id}/map_players` | Map players to pitch |&#10;| POST | `/videos/{id}/interpolate` | Interpolate trajectories |&#10;| GET | `/videos/{id}/players` | Get player positions |&#10;| POST | `/process-video` | Full pipeline (single request) |&#10;&#10;### Interactive Documentation&#10;&#10;- Swagger UI: `http://localhost:8000/docs`&#10;- ReDoc: `http://localhost:8000/redoc`&#10;&#10;## Configuration&#10;&#10;Environment variables (with defaults):&#10;&#10;| Variable | Default | Description |&#10;|----------|---------|-------------|&#10;| `PORT` | `8000` | Server port |&#10;| `ALLOWED_ORIGINS` | `*` | CORS origins (comma-separated) |&#10;| `MAX_VIDEO_SIZE_MB` | `500` | Max upload size |&#10;| `DATA_DIR` | `data` | Data storage directory |&#10;| `YOLO_MODEL_PATH` | `models/best.pt` | Path to YOLO model |&#10;&#10;## Project Structure&#10;&#10;```&#10;interactive_analytics_system_backend/&#10;├── app.py                 # FastAPI application&#10;├── main.py                # Uvicorn entry point&#10;├── pipeline/              # Core processing modules&#10;│   ├── __init__.py&#10;│   ├── config.py          # Configuration constants&#10;│   ├── detect.py          # YOLO + ByteTrack detection&#10;│   ├── gaa_pitch_config.py # GAA pitch vertices&#10;│   ├── homography.py      # Homography computation&#10;│   ├── map_players.py     # Pixel to pitch mapping&#10;│   ├── schemas.py         # Pydantic models&#10;│   ├── trajectories.py    # Trajectory interpolation&#10;│   └── video.py           # Video utilities&#10;├── models/                # YOLO model weights&#10;│   └── best.pt&#10;├── tests/                 # Test suite&#10;├── requirements.txt       # Production dependencies&#10;├── requirements-dev.txt   # Development dependencies&#10;├── Dockerfile             # Container configuration&#10;├── render.yaml            # Render deployment config&#10;└── .env.example           # Environment template&#10;```&#10;&#10;## Testing&#10;&#10;```bash&#10;# Run all tests&#10;pytest -v&#10;&#10;# Run with coverage&#10;pytest --cov=pipeline --cov-report=html&#10;```&#10;&#10;## Data Contract&#10;&#10;The API uses these core data structures:&#10;&#10;1. **Detection** - YOLO tracking output&#10;   ```json&#10;   {&quot;frame_idx&quot;: 0, &quot;track_id&quot;: 1, &quot;x1&quot;: 100, &quot;y1&quot;: 100, &quot;x2&quot;: 150, &quot;y2&quot;: 200, &quot;confidence&quot;: 0.9}&#10;   ```&#10;&#10;2. **PitchAnnotation** - Keypoint annotations&#10;   ```json&#10;   {&quot;frame_idx&quot;: 0, &quot;points&quot;: [{&quot;pitch_id&quot;: &quot;corner_tl&quot;, &quot;x_img&quot;: 100, &quot;y_img&quot;: 50}]}&#10;   ```&#10;&#10;3. **PlayerPitchPosition** - Mapped positions&#10;   ```json&#10;   {&quot;frame_idx&quot;: 0, &quot;track_id&quot;: 1, &quot;x_pitch&quot;: 425.0, &quot;y_pitch&quot;: 725.0, &quot;source&quot;: &quot;homography&quot;}&#10;   ```&#10;&#10;## License&#10;&#10;MIT&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/interactive_analytics_system_backend/app.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/interactive_analytics_system_backend/app.py" />
              <option name="originalContent" value="&quot;&quot;&quot;FastAPI application for video analysis pipeline.&quot;&quot;&quot;&#10;import os&#10;import uuid&#10;from pathlib import Path&#10;from typing import List, Dict, Optional&#10;import json&#10;import numpy as np&#10;&#10;from fastapi import FastAPI, UploadFile, File, HTTPException, Query, Form&#10;from fastapi.responses import JSONResponse&#10;from fastapi.middleware.cors import CORSMiddleware&#10;&#10;from pipeline.schemas import (&#10;    VideoCreateResponse,&#10;    PitchAnnotation,&#10;    TrackResponse,&#10;    HomographyResponse,&#10;    InterpolationResponse,&#10;    Detection,&#10;    PlayerPitchPosition,&#10;    ProcessVideoResponse&#10;)&#10;from pipeline.detect import run_tracking&#10;from pipeline.homography import compute_homographies_from_annotations&#10;from pipeline.map_players import map_players_to_pitch&#10;from pipeline.trajectories import interpolate_trajectories&#10;from pipeline.video import get_video_metadata&#10;&#10;app = FastAPI(title=&quot;GAA Video Analysis API&quot;)&#10;&#10;# Enable CORS for frontend (Vercel)&#10;app.add_middleware(&#10;    CORSMiddleware,&#10;    allow_origins=[&quot;*&quot;],  # In production, specify your Vercel URL&#10;    allow_credentials=True,&#10;    allow_methods=[&quot;*&quot;],&#10;    allow_headers=[&quot;*&quot;],&#10;)&#10;&#10;# Data directories&#10;DATA_DIR = Path(&quot;data&quot;)&#10;VIDEOS_DIR = DATA_DIR / &quot;videos&quot;&#10;TRACKS_DIR = DATA_DIR / &quot;tracks&quot;&#10;ANNOTATIONS_DIR = DATA_DIR / &quot;annotations&quot;&#10;&#10;# Create directories if they don't exist&#10;VIDEOS_DIR.mkdir(parents=True, exist_ok=True)&#10;TRACKS_DIR.mkdir(parents=True, exist_ok=True)&#10;ANNOTATIONS_DIR.mkdir(parents=True, exist_ok=True)&#10;&#10;# In-memory storage (could be replaced with database)&#10;videos: Dict[str, dict] = {}&#10;detections_cache: Dict[str, List[Detection]] = {}&#10;homographies_cache: Dict[str, Dict[int, any]] = {}&#10;player_positions_cache: Dict[str, List[PlayerPitchPosition]] = {}&#10;&#10;&#10;def save_detections(video_id: str, detections: List[Detection]):&#10;    &quot;&quot;&quot;Save detections to JSON file.&quot;&quot;&quot;&#10;    file_path = TRACKS_DIR / f&quot;{video_id}.json&quot;&#10;    with open(file_path, &quot;w&quot;) as f:&#10;        json.dump([d.dict() for d in detections], f, indent=2)&#10;&#10;&#10;def load_detections(video_id: str) -&gt; Optional[List[Detection]]:&#10;    &quot;&quot;&quot;Load detections from JSON file.&quot;&quot;&quot;&#10;    file_path = TRACKS_DIR / f&quot;{video_id}.json&quot;&#10;    if file_path.exists():&#10;        with open(file_path, &quot;r&quot;) as f:&#10;            data = json.load(f)&#10;            return [Detection(**d) for d in data]&#10;    return None&#10;&#10;&#10;def save_homographies(video_id: str, homographies: Dict[int, any]):&#10;    &quot;&quot;&quot;Save homographies to JSON file (as lists for JSON serialization).&quot;&quot;&quot;&#10;    file_path = ANNOTATIONS_DIR / f&quot;{video_id}_homographies.json&quot;&#10;    homographies_serialized = {&#10;        str(k): v.tolist() for k, v in homographies.items()&#10;    }&#10;    with open(file_path, &quot;w&quot;) as f:&#10;        json.dump(homographies_serialized, f, indent=2)&#10;&#10;&#10;def load_homographies(video_id: str) -&gt; Optional[Dict[int, any]]:&#10;    &quot;&quot;&quot;Load homographies from JSON file.&quot;&quot;&quot;&#10;    file_path = ANNOTATIONS_DIR / f&quot;{video_id}_homographies.json&quot;&#10;    if file_path.exists():&#10;        with open(file_path, &quot;r&quot;) as f:&#10;            data = json.load(f)&#10;            return {int(k): np.array(v) for k, v in data.items()}&#10;    return None&#10;&#10;&#10;@app.post(&quot;/videos&quot;, response_model=VideoCreateResponse)&#10;async def upload_video(file: UploadFile = File(...)):&#10;    &quot;&quot;&quot;&#10;    Upload a video file and extract metadata.&#10;    &#10;    Returns video_id, fps, and num_frames.&#10;    &quot;&quot;&quot;&#10;    # Generate unique video ID&#10;    video_id = str(uuid.uuid4())&#10;    &#10;    # Save video file&#10;    video_path = VIDEOS_DIR / f&quot;{video_id}.mp4&quot;&#10;    with open(video_path, &quot;wb&quot;) as f:&#10;        content = await file.read()&#10;        f.write(content)&#10;    &#10;    # Extract metadata&#10;    try:&#10;        metadata = get_video_metadata(str(video_path))&#10;    except Exception as e:&#10;        # Clean up file on error&#10;        video_path.unlink()&#10;        raise HTTPException(status_code=400, detail=f&quot;Failed to process video: {str(e)}&quot;)&#10;&#10;    # Store video info&#10;    videos[video_id] = {&#10;        &quot;path&quot;: str(video_path),&#10;        &quot;fps&quot;: metadata[&quot;fps&quot;],&#10;        &quot;num_frames&quot;: metadata[&quot;num_frames&quot;]&#10;    }&#10;    &#10;    return VideoCreateResponse(&#10;        video_id=video_id,&#10;        fps=metadata[&quot;fps&quot;],&#10;        num_frames=metadata[&quot;num_frames&quot;]&#10;    )&#10;&#10;&#10;@app.post(&quot;/videos/{video_id}/track&quot;, response_model=TrackResponse)&#10;async def track_video(video_id: str):&#10;    &quot;&quot;&quot;&#10;    Run YOLO + ByteTrack tracking on the video.&#10;    &#10;    Returns number of frames processed and unique tracks detected.&#10;    &quot;&quot;&quot;&#10;    if video_id not in videos:&#10;        raise HTTPException(status_code=404, detail=&quot;Video not found&quot;)&#10;    &#10;    video_path = videos[video_id][&quot;path&quot;]&#10;    &#10;    # Check cache first&#10;    detections = load_detections(video_id)&#10;    if detections is None:&#10;        # Run tracking&#10;        try:&#10;            detections = run_tracking(video_path)&#10;            detections_cache[video_id] = detections&#10;            save_detections(video_id, detections)&#10;        except Exception as e:&#10;            raise HTTPException(status_code=500, detail=f&quot;Tracking failed: {str(e)}&quot;)&#10;    else:&#10;        detections_cache[video_id] = detections&#10;    &#10;    if not detections:&#10;        raise HTTPException(status_code=500, detail=&quot;No detections found&quot;)&#10;&#10;    # Count unique tracks&#10;    unique_tracks = len(set(d.track_id for d in detections))&#10;    &#10;    # Count frames processed&#10;    frames_processed = max(d.frame_idx for d in detections) + 1&#10;    &#10;    return TrackResponse(&#10;        frames_processed=frames_processed,&#10;        tracks=unique_tracks&#10;    )&#10;&#10;&#10;@app.post(&quot;/videos/{video_id}/homographies&quot;, response_model=HomographyResponse)&#10;async def compute_homographies(&#10;    video_id: str,&#10;    annotations: List[PitchAnnotation]&#10;):&#10;    &quot;&quot;&quot;&#10;    Compute homography matrices from pitch annotations.&#10;    &#10;    Accepts a list of PitchAnnotation objects (one per frame).&#10;    Returns list of frame indices for which homographies were computed.&#10;    &quot;&quot;&quot;&#10;    if video_id not in videos:&#10;        raise HTTPException(status_code=404, detail=&quot;Video not found&quot;)&#10;    &#10;    # Convert annotations to dict format expected by compute_homographies_from_annotations&#10;    annotations_dict = {}&#10;    for ann in annotations:&#10;        annotations_dict[ann.frame_idx] = ann.points&#10;    &#10;    # Compute homographies&#10;    try:&#10;        homographies = compute_homographies_from_annotations(annotations_dict)&#10;        homographies_cache[video_id] = homographies&#10;        save_homographies(video_id, homographies)&#10;    except Exception as e:&#10;        raise HTTPException(status_code=500, detail=f&quot;Homography computation failed: {str(e)}&quot;)&#10;&#10;    if not homographies:&#10;        raise HTTPException(status_code=400, detail=&quot;No valid homographies computed&quot;)&#10;&#10;    return HomographyResponse(frames=sorted(homographies.keys()))&#10;&#10;&#10;@app.post(&quot;/videos/{video_id}/map_players&quot;, response_model=List[PlayerPitchPosition])&#10;async def map_players(video_id: str):&#10;    &quot;&quot;&quot;&#10;    Map player detections to pitch coordinates using computed homographies.&#10;    &#10;    Returns list of PlayerPitchPosition objects with source=&quot;homography&quot;.&#10;    &quot;&quot;&quot;&#10;    if video_id not in videos:&#10;        raise HTTPException(status_code=404, detail=&quot;Video not found&quot;)&#10;    &#10;    # Load detections&#10;    detections = load_detections(video_id)&#10;    if detections is None:&#10;        raise HTTPException(status_code=400, detail=&quot;No detections found. Run tracking first.&quot;)&#10;    &#10;    # Load homographies&#10;    homographies = load_homographies(video_id)&#10;    if homographies is None:&#10;        raise HTTPException(status_code=400, detail=&quot;No homographies found. Compute homographies first.&quot;)&#10;    &#10;    # Map players to pitch&#10;    try:&#10;        positions = map_players_to_pitch(detections, homographies)&#10;        player_positions_cache[video_id] = positions&#10;    except Exception as e:&#10;        raise HTTPException(status_code=500, detail=f&quot;Player mapping failed: {str(e)}&quot;)&#10;&#10;    return positions&#10;&#10;&#10;@app.post(&quot;/videos/{video_id}/interpolate&quot;, response_model=InterpolationResponse)&#10;async def interpolate_trajectories_endpoint(&#10;    video_id: str,&#10;    start_frame: int = Query(0, description=&quot;First frame to interpolate&quot;),&#10;    end_frame: int = Query(100, description=&quot;Last frame to interpolate (inclusive)&quot;)&#10;):&#10;    &quot;&quot;&quot;&#10;    Interpolate player trajectories between anchor frames.&#10;    &#10;    Args:&#10;        video_id: Video identifier&#10;        start_frame: First frame to interpolate&#10;        end_frame: Last frame to interpolate (inclusive)&#10;    &#10;    Returns:&#10;        InterpolationResponse with number of frames generated&#10;    &quot;&quot;&quot;&#10;    if video_id not in videos:&#10;        raise HTTPException(status_code=404, detail=&quot;Video not found&quot;)&#10;    &#10;    # Get sparse positions (from mapping)&#10;    sparse_positions = player_positions_cache.get(video_id)&#10;    if sparse_positions is None:&#10;        raise HTTPException(&#10;            status_code=400,&#10;            detail=&quot;No player positions found. Run map_players first.&quot;&#10;        )&#10;    &#10;    # Filter to only homography-sourced positions&#10;    homography_positions = [&#10;        p for p in sparse_positions if p.source == &quot;homography&quot;&#10;    ]&#10;    &#10;    if not homography_positions:&#10;        raise HTTPException(&#10;            status_code=400,&#10;            detail=&quot;No homography-based positions found for interpolation&quot;&#10;        )&#10;    &#10;    # Interpolate&#10;    try:&#10;        interpolated = interpolate_trajectories(&#10;            homography_positions,&#10;            start_frame,&#10;            end_frame&#10;        )&#10;        &#10;        # Update cache with interpolated positions&#10;        # Remove old interpolated positions in range&#10;        existing = player_positions_cache.get(video_id, [])&#10;        existing_filtered = [&#10;            p for p in existing&#10;            if not (start_frame &lt;= p.frame_idx &lt;= end_frame and p.source == &quot;interpolated&quot;)&#10;        ]&#10;        &#10;        # Add new interpolated positions&#10;        player_positions_cache[video_id] = existing_filtered + interpolated&#10;        &#10;        # Count newly generated frames&#10;        frames_generated = len([&#10;            p for p in interpolated if p.source == &quot;interpolated&quot;&#10;        ])&#10;        &#10;    except Exception as e:&#10;        raise HTTPException(status_code=500, detail=f&quot;Interpolation failed: {str(e)}&quot;)&#10;&#10;    return InterpolationResponse(&#10;        frames_generated=frames_generated,&#10;        method=&quot;linear&quot;&#10;    )&#10;&#10;&#10;@app.get(&quot;/videos/{video_id}/players&quot;, response_model=List[PlayerPitchPosition])&#10;async def get_player_positions(video_id: str):&#10;    &quot;&quot;&quot;&#10;    Get all player positions (sparse + interpolated) for a video.&#10;    &#10;    Returns list of PlayerPitchPosition objects sorted by frame_idx and track_id.&#10;    &quot;&quot;&quot;&#10;    if video_id not in videos:&#10;        raise HTTPException(status_code=404, detail=&quot;Video not found&quot;)&#10;    &#10;    positions = player_positions_cache.get(video_id)&#10;    if positions is None:&#10;        raise HTTPException(&#10;            status_code=404,&#10;            detail=&quot;No player positions found. Run map_players first.&quot;&#10;        )&#10;    &#10;    # Sort by frame_idx, then track_id&#10;    positions_sorted = sorted(positions, key=lambda p: (p.frame_idx, p.track_id))&#10;    &#10;    return positions_sorted&#10;&#10;&#10;@app.post(&quot;/process-video&quot;, response_model=ProcessVideoResponse)&#10;async def process_video(&#10;    file: UploadFile = File(...),&#10;    annotations_json: str = Form(..., description=&quot;JSON string of pitch annotations for anchor frames&quot;)&#10;):&#10;    &quot;&quot;&quot;&#10;    Unified endpoint to process a video through the entire pipeline.&#10;    &#10;    Steps:&#10;    1. Upload video&#10;    2. Run YOLOv8n + ByteTrack tracking&#10;    3. Compute homographies at anchor frames&#10;    4. Map players to pitch coordinates&#10;    5. Interpolate player trajectories&#10;    6. Return JSON pitch coordinates&#10;    &#10;    Args:&#10;        file: MP4 video file&#10;        annotations: List of pitch annotations for anchor frames&#10;&#10;    Returns:&#10;        ProcessVideoResponse with video_id, status, and player_positions&#10;    &quot;&quot;&quot;&#10;    # Step 1: Upload video&#10;    video_id = str(uuid.uuid4())&#10;    video_path = VIDEOS_DIR / f&quot;{video_id}.mp4&quot;&#10;    &#10;    try:&#10;        with open(video_path, &quot;wb&quot;) as f:&#10;            content = await file.read()&#10;            f.write(content)&#10;        &#10;        # Extract metadata&#10;        metadata = get_video_metadata(str(video_path))&#10;        videos[video_id] = {&#10;            &quot;path&quot;: str(video_path),&#10;            &quot;fps&quot;: metadata[&quot;fps&quot;],&#10;            &quot;num_frames&quot;: metadata[&quot;num_frames&quot;]&#10;        }&#10;    except Exception as e:&#10;        if video_path.exists():&#10;            video_path.unlink()&#10;        raise HTTPException(status_code=400, detail=f&quot;Failed to upload video: {str(e)}&quot;)&#10;&#10;    try:&#10;        # Step 2: Run YOLOv8n + ByteTrack tracking&#10;        detections = run_tracking(str(video_path))&#10;        if not detections:&#10;            raise HTTPException(status_code=500, detail=&quot;No detections found&quot;)&#10;&#10;        detections_cache[video_id] = detections&#10;        save_detections(video_id, detections)&#10;        &#10;        # Step 3: Compute homographies at anchor frames&#10;        # Parse annotations JSON&#10;        try:&#10;            annotations_list = json.loads(annotations_json)&#10;            annotations = [PitchAnnotation(**ann) for ann in annotations_list]&#10;        except Exception as e:&#10;            raise HTTPException(status_code=400, detail=f&quot;Invalid annotations format: {str(e)}&quot;)&#10;&#10;        annotations_dict = {}&#10;        for ann in annotations:&#10;            annotations_dict[ann.frame_idx] = ann.points&#10;        &#10;        homographies = compute_homographies_from_annotations(annotations_dict)&#10;        if not homographies:&#10;            raise HTTPException(status_code=400, detail=&quot;No valid homographies computed. Need at least 4 points per frame.&quot;)&#10;        &#10;        homographies_cache[video_id] = homographies&#10;        save_homographies(video_id, homographies)&#10;        &#10;        # Step 4: Map players to pitch coordinates&#10;        positions = map_players_to_pitch(detections, homographies)&#10;        if not positions:&#10;            raise HTTPException(status_code=500, detail=&quot;Failed to map players to pitch&quot;)&#10;        &#10;        player_positions_cache[video_id] = positions&#10;        &#10;        # Step 5: Interpolate trajectories&#10;        # Get frame range from video metadata&#10;        num_frames = metadata[&quot;num_frames&quot;]&#10;        interpolated = interpolate_trajectories(positions, 0, num_frames - 1)&#10;        &#10;        # Combine sparse and interpolated positions&#10;        all_positions = positions + [&#10;            p for p in interpolated if p.source == &quot;interpolated&quot;&#10;        ]&#10;        &#10;        # Sort by frame_idx, then track_id&#10;        all_positions_sorted = sorted(all_positions, key=lambda p: (p.frame_idx, p.track_id))&#10;        player_positions_cache[video_id] = all_positions_sorted&#10;        &#10;        return ProcessVideoResponse(&#10;            video_id=video_id,&#10;            status=&quot;completed&quot;,&#10;            player_positions=all_positions_sorted&#10;        )&#10;        &#10;    except HTTPException:&#10;        raise&#10;    except Exception as e:&#10;        # Cleanup on error&#10;        if video_path.exists():&#10;            video_path.unlink()&#10;        raise HTTPException(status_code=500, detail=f&quot;Processing failed: {str(e)}&quot;)&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;FastAPI application for video analysis pipeline.&quot;&quot;&quot;&#10;import os&#10;import uuid&#10;from pathlib import Path&#10;from typing import List, Dict, Optional&#10;import json&#10;import numpy as np&#10;import logging&#10;&#10;from fastapi import FastAPI, UploadFile, File, HTTPException, Query, Form&#10;from fastapi.middleware.cors import CORSMiddleware&#10;&#10;from pipeline.schemas import (&#10;    VideoCreateResponse,&#10;    PitchAnnotation,&#10;    TrackResponse,&#10;    HomographyResponse,&#10;    InterpolationResponse,&#10;    Detection,&#10;    PlayerPitchPosition,&#10;    ProcessVideoResponse&#10;)&#10;# NOTE: `run_tracking` performs heavy ML imports; import lazily inside endpoints to keep module import lightweight.&#10;# from pipeline.detect import run_tracking&#10;from pipeline.homography import compute_homographies_from_annotations&#10;from pipeline.map_players import map_players_to_pitch&#10;from pipeline.trajectories import interpolate_trajectories&#10;from pipeline.video import get_video_metadata&#10;&#10;# Configure logging&#10;logging.basicConfig(level=logging.INFO)&#10;logger = logging.getLogger(__name__)&#10;&#10;# Configuration from environment variables&#10;MAX_VIDEO_SIZE_MB = int(os.getenv(&quot;MAX_VIDEO_SIZE_MB&quot;, &quot;500&quot;))&#10;MAX_VIDEO_SIZE = MAX_VIDEO_SIZE_MB * 1024 * 1024  # Convert to bytes&#10;ALLOWED_ORIGINS = os.getenv(&quot;ALLOWED_ORIGINS&quot;, &quot;*&quot;).split(&quot;,&quot;)&#10;DATA_DIR = Path(os.getenv(&quot;DATA_DIR&quot;, &quot;data&quot;))&#10;&#10;app = FastAPI(title=&quot;GAA Video Analysis API&quot;)&#10;&#10;# Enable CORS for frontend&#10;app.add_middleware(&#10;    CORSMiddleware,&#10;    allow_origins=ALLOWED_ORIGINS,&#10;    allow_credentials=True,&#10;    allow_methods=[&quot;*&quot;],&#10;    allow_headers=[&quot;*&quot;],&#10;)&#10;&#10;# Data directories&#10;VIDEOS_DIR = DATA_DIR / &quot;videos&quot;&#10;TRACKS_DIR = DATA_DIR / &quot;tracks&quot;&#10;ANNOTATIONS_DIR = DATA_DIR / &quot;annotations&quot;&#10;&#10;# Create directories if they don't exist&#10;VIDEOS_DIR.mkdir(parents=True, exist_ok=True)&#10;TRACKS_DIR.mkdir(parents=True, exist_ok=True)&#10;ANNOTATIONS_DIR.mkdir(parents=True, exist_ok=True)&#10;&#10;# In-memory storage (could be replaced with database)&#10;videos: Dict[str, dict] = {}&#10;detections_cache: Dict[str, List[Detection]] = {}&#10;homographies_cache: Dict[str, Dict[int, any]] = {}&#10;player_positions_cache: Dict[str, List[PlayerPitchPosition]] = {}&#10;&#10;&#10;# --- Health Check ---&#10;@app.get(&quot;/health&quot;)&#10;async def health_check():&#10;    &quot;&quot;&quot;Health check endpoint for container orchestration.&quot;&quot;&quot;&#10;    return {&quot;status&quot;: &quot;ok&quot;}&#10;&#10;&#10;# --- Helper Functions ---&#10;def validate_video_upload(file: UploadFile, content: bytes) -&gt; None:&#10;    &quot;&quot;&quot;Validate uploaded video file.&quot;&quot;&quot;&#10;    # Check file size&#10;    if len(content) &gt; MAX_VIDEO_SIZE:&#10;        raise HTTPException(&#10;            status_code=413,&#10;            detail=f&quot;File too large. Maximum size is {MAX_VIDEO_SIZE_MB}MB&quot;&#10;        )&#10;&#10;    # Check file extension&#10;    if not file.filename or not file.filename.lower().endswith(&quot;.mp4&quot;):&#10;        raise HTTPException(&#10;            status_code=400,&#10;            detail=&quot;Only MP4 video files are accepted&quot;&#10;        )&#10;&#10;    # Check MIME type if provided&#10;    if file.content_type and file.content_type not in [&quot;video/mp4&quot;, &quot;application/octet-stream&quot;]:&#10;        raise HTTPException(&#10;            status_code=400,&#10;            detail=f&quot;Invalid content type: {file.content_type}. Expected video/mp4&quot;&#10;        )&#10;&#10;&#10;def save_detections(video_id: str, detections: List[Detection]):&#10;    &quot;&quot;&quot;Save detections to JSON file.&quot;&quot;&quot;&#10;    file_path = TRACKS_DIR / f&quot;{video_id}.json&quot;&#10;    with open(file_path, &quot;w&quot;) as f:&#10;        json.dump([d.model_dump() for d in detections], f, indent=2)&#10;&#10;&#10;def load_detections(video_id: str) -&gt; Optional[List[Detection]]:&#10;    &quot;&quot;&quot;Load detections from JSON file.&quot;&quot;&quot;&#10;    file_path = TRACKS_DIR / f&quot;{video_id}.json&quot;&#10;    if file_path.exists():&#10;        with open(file_path, &quot;r&quot;) as f:&#10;            data = json.load(f)&#10;            return [Detection(**d) for d in data]&#10;    return None&#10;&#10;&#10;def save_homographies(video_id: str, homographies: Dict[int, any]):&#10;    &quot;&quot;&quot;Save homographies to JSON file (as lists for JSON serialization).&quot;&quot;&quot;&#10;    file_path = ANNOTATIONS_DIR / f&quot;{video_id}_homographies.json&quot;&#10;    homographies_serialized = {&#10;        str(k): v.tolist() for k, v in homographies.items()&#10;    }&#10;    with open(file_path, &quot;w&quot;) as f:&#10;        json.dump(homographies_serialized, f, indent=2)&#10;&#10;&#10;def load_homographies(video_id: str) -&gt; Optional[Dict[int, any]]:&#10;    &quot;&quot;&quot;Load homographies from JSON file.&quot;&quot;&quot;&#10;    file_path = ANNOTATIONS_DIR / f&quot;{video_id}_homographies.json&quot;&#10;    if file_path.exists():&#10;        with open(file_path, &quot;r&quot;) as f:&#10;            data = json.load(f)&#10;            return {int(k): np.array(v) for k, v in data.items()}&#10;    return None&#10;&#10;&#10;def sanitize_error_message(error: Exception) -&gt; str:&#10;    &quot;&quot;&quot;Sanitize error messages for client responses.&quot;&quot;&quot;&#10;    # In production, you may want to log the full error and return a generic message&#10;    error_str = str(error)&#10;    # Remove file paths from error messages&#10;    if &quot;data/&quot; in error_str or &quot;/Users/&quot; in error_str or &quot;/home/&quot; in error_str:&#10;        return &quot;An internal error occurred&quot;&#10;    return error_str&#10;&#10;&#10;# --- Endpoints ---&#10;@app.post(&quot;/videos&quot;, response_model=VideoCreateResponse)&#10;async def upload_video(file: UploadFile = File(...)):&#10;    &quot;&quot;&quot;&#10;    Upload a video file and extract metadata.&#10;    &#10;    Returns video_id, fps, and num_frames.&#10;    &quot;&quot;&quot;&#10;    # Read file content&#10;    content = await file.read()&#10;&#10;    # Validate upload&#10;    validate_video_upload(file, content)&#10;&#10;    # Generate unique video ID&#10;    video_id = str(uuid.uuid4())&#10;    &#10;    # Save video file&#10;    video_path = VIDEOS_DIR / f&quot;{video_id}.mp4&quot;&#10;    with open(video_path, &quot;wb&quot;) as f:&#10;        f.write(content)&#10;    &#10;    # Extract metadata&#10;    try:&#10;        metadata = get_video_metadata(str(video_path))&#10;    except Exception as e:&#10;        # Clean up file on error&#10;        video_path.unlink()&#10;        logger.error(f&quot;Failed to process video {video_id}: {e}&quot;)&#10;        raise HTTPException(status_code=400, detail=&quot;Failed to process video. Ensure it is a valid MP4 file.&quot;)&#10;&#10;    # Store video info&#10;    videos[video_id] = {&#10;        &quot;path&quot;: str(video_path),&#10;        &quot;fps&quot;: metadata[&quot;fps&quot;],&#10;        &quot;num_frames&quot;: metadata[&quot;num_frames&quot;]&#10;    }&#10;    &#10;    logger.info(f&quot;Uploaded video {video_id}: {metadata['num_frames']} frames at {metadata['fps']} fps&quot;)&#10;&#10;    return VideoCreateResponse(&#10;        video_id=video_id,&#10;        fps=metadata[&quot;fps&quot;],&#10;        num_frames=metadata[&quot;num_frames&quot;]&#10;    )&#10;&#10;&#10;@app.post(&quot;/videos/{video_id}/track&quot;, response_model=TrackResponse)&#10;async def track_video(video_id: str):&#10;    &quot;&quot;&quot;&#10;    Run YOLO + ByteTrack tracking on the video.&#10;    &#10;    Returns number of frames processed and unique tracks detected.&#10;    &quot;&quot;&quot;&#10;    if video_id not in videos:&#10;        raise HTTPException(status_code=404, detail=&quot;Video not found&quot;)&#10;    &#10;    video_path = videos[video_id][&quot;path&quot;]&#10;    &#10;    # Check cache first&#10;    detections = load_detections(video_id)&#10;    if detections is None:&#10;        # Run tracking&#10;        try:&#10;            from pipeline.detect import run_tracking  # Import here to avoid top-level ML imports&#10;            logger.info(f&quot;Running tracking on video {video_id}&quot;)&#10;            detections = run_tracking(video_path)&#10;            detections_cache[video_id] = detections&#10;            save_detections(video_id, detections)&#10;        except Exception as e:&#10;            logger.error(f&quot;Tracking failed for video {video_id}: {e}&quot;)&#10;            raise HTTPException(status_code=500, detail=&quot;Tracking failed. Please try again.&quot;)&#10;    else:&#10;        detections_cache[video_id] = detections&#10;    &#10;    if not detections:&#10;        raise HTTPException(status_code=500, detail=&quot;No detections found in video&quot;)&#10;&#10;    # Count unique tracks&#10;    unique_tracks = len(set(d.track_id for d in detections))&#10;    &#10;    # Count frames processed&#10;    frames_processed = max(d.frame_idx for d in detections) + 1&#10;    &#10;    logger.info(f&quot;Tracking complete for {video_id}: {frames_processed} frames, {unique_tracks} tracks&quot;)&#10;&#10;    return TrackResponse(&#10;        frames_processed=frames_processed,&#10;        tracks=unique_tracks&#10;    )&#10;&#10;&#10;@app.post(&quot;/videos/{video_id}/homographies&quot;, response_model=HomographyResponse)&#10;async def compute_homographies(&#10;    video_id: str,&#10;    annotations: List[PitchAnnotation]&#10;):&#10;    &quot;&quot;&quot;&#10;    Compute homography matrices from pitch annotations.&#10;    &#10;    Accepts a list of PitchAnnotation objects (one per frame).&#10;    Returns list of frame indices for which homographies were computed.&#10;    &quot;&quot;&quot;&#10;    if video_id not in videos:&#10;        raise HTTPException(status_code=404, detail=&quot;Video not found&quot;)&#10;    &#10;    # Convert annotations to dict format expected by compute_homographies_from_annotations&#10;    annotations_dict = {}&#10;    for ann in annotations:&#10;        annotations_dict[ann.frame_idx] = ann.points&#10;    &#10;    # Compute homographies&#10;    try:&#10;        homographies = compute_homographies_from_annotations(annotations_dict)&#10;        homographies_cache[video_id] = homographies&#10;        save_homographies(video_id, homographies)&#10;    except Exception as e:&#10;        logger.error(f&quot;Homography computation failed for video {video_id}: {e}&quot;)&#10;        raise HTTPException(status_code=500, detail=&quot;Homography computation failed&quot;)&#10;&#10;    if not homographies:&#10;        raise HTTPException(status_code=400, detail=&quot;No valid homographies computed. Need at least 4 points per frame.&quot;)&#10;&#10;    logger.info(f&quot;Computed {len(homographies)} homographies for video {video_id}&quot;)&#10;&#10;    return HomographyResponse(frames=sorted(homographies.keys()))&#10;&#10;&#10;@app.post(&quot;/videos/{video_id}/map_players&quot;, response_model=List[PlayerPitchPosition])&#10;async def map_players(video_id: str):&#10;    &quot;&quot;&quot;&#10;    Map player detections to pitch coordinates using computed homographies.&#10;    &#10;    Returns list of PlayerPitchPosition objects with source=&quot;homography&quot;.&#10;    &quot;&quot;&quot;&#10;    if video_id not in videos:&#10;        raise HTTPException(status_code=404, detail=&quot;Video not found&quot;)&#10;    &#10;    # Load detections&#10;    detections = load_detections(video_id)&#10;    if detections is None:&#10;        raise HTTPException(status_code=400, detail=&quot;No detections found. Run tracking first.&quot;)&#10;    &#10;    # Load homographies&#10;    homographies = load_homographies(video_id)&#10;    if homographies is None:&#10;        raise HTTPException(status_code=400, detail=&quot;No homographies found. Compute homographies first.&quot;)&#10;    &#10;    # Map players to pitch&#10;    try:&#10;        positions = map_players_to_pitch(detections, homographies)&#10;        player_positions_cache[video_id] = positions&#10;    except Exception as e:&#10;        logger.error(f&quot;Player mapping failed for video {video_id}: {e}&quot;)&#10;        raise HTTPException(status_code=500, detail=&quot;Player mapping failed&quot;)&#10;&#10;    logger.info(f&quot;Mapped {len(positions)} player positions for video {video_id}&quot;)&#10;&#10;    return positions&#10;&#10;&#10;@app.post(&quot;/videos/{video_id}/interpolate&quot;, response_model=InterpolationResponse)&#10;async def interpolate_trajectories_endpoint(&#10;    video_id: str,&#10;    start_frame: int = Query(0, description=&quot;First frame to interpolate&quot;),&#10;    end_frame: int = Query(100, description=&quot;Last frame to interpolate (inclusive)&quot;)&#10;):&#10;    &quot;&quot;&quot;&#10;    Interpolate player trajectories between anchor frames.&#10;    &#10;    Args:&#10;        video_id: Video identifier&#10;        start_frame: First frame to interpolate&#10;        end_frame: Last frame to interpolate (inclusive)&#10;    &#10;    Returns:&#10;        InterpolationResponse with number of frames generated&#10;    &quot;&quot;&quot;&#10;    if video_id not in videos:&#10;        raise HTTPException(status_code=404, detail=&quot;Video not found&quot;)&#10;    &#10;    # Validate frame range&#10;    if start_frame &lt; 0 or end_frame &lt; start_frame:&#10;        raise HTTPException(status_code=400, detail=&quot;Invalid frame range&quot;)&#10;&#10;    # Get sparse positions (from mapping)&#10;    sparse_positions = player_positions_cache.get(video_id)&#10;    if sparse_positions is None:&#10;        raise HTTPException(&#10;            status_code=400,&#10;            detail=&quot;No player positions found. Run map_players first.&quot;&#10;        )&#10;    &#10;    # Filter to only homography-sourced positions&#10;    homography_positions = [&#10;        p for p in sparse_positions if p.source == &quot;homography&quot;&#10;    ]&#10;    &#10;    if not homography_positions:&#10;        raise HTTPException(&#10;            status_code=400,&#10;            detail=&quot;No homography-based positions found for interpolation&quot;&#10;        )&#10;    &#10;    # Interpolate&#10;    try:&#10;        interpolated = interpolate_trajectories(&#10;            homography_positions,&#10;            start_frame,&#10;            end_frame&#10;        )&#10;        &#10;        # Update cache with interpolated positions&#10;        # Remove old interpolated positions in range&#10;        existing = player_positions_cache.get(video_id, [])&#10;        existing_filtered = [&#10;            p for p in existing&#10;            if not (start_frame &lt;= p.frame_idx &lt;= end_frame and p.source == &quot;interpolated&quot;)&#10;        ]&#10;        &#10;        # Add new interpolated positions&#10;        player_positions_cache[video_id] = existing_filtered + interpolated&#10;        &#10;        # Count newly generated frames&#10;        frames_generated = len([&#10;            p for p in interpolated if p.source == &quot;interpolated&quot;&#10;        ])&#10;        &#10;    except Exception as e:&#10;        logger.error(f&quot;Interpolation failed for video {video_id}: {e}&quot;)&#10;        raise HTTPException(status_code=500, detail=&quot;Interpolation failed&quot;)&#10;&#10;    logger.info(f&quot;Generated {frames_generated} interpolated frames for video {video_id}&quot;)&#10;&#10;    return InterpolationResponse(&#10;        frames_generated=frames_generated,&#10;        method=&quot;linear&quot;&#10;    )&#10;&#10;&#10;@app.get(&quot;/videos/{video_id}/players&quot;, response_model=List[PlayerPitchPosition])&#10;async def get_player_positions(video_id: str):&#10;    &quot;&quot;&quot;&#10;    Get all player positions (sparse + interpolated) for a video.&#10;    &#10;    Returns list of PlayerPitchPosition objects sorted by frame_idx and track_id.&#10;    &quot;&quot;&quot;&#10;    if video_id not in videos:&#10;        raise HTTPException(status_code=404, detail=&quot;Video not found&quot;)&#10;    &#10;    positions = player_positions_cache.get(video_id)&#10;    if positions is None:&#10;        raise HTTPException(&#10;            status_code=404,&#10;            detail=&quot;No player positions found. Run map_players first.&quot;&#10;        )&#10;    &#10;    # Sort by frame_idx, then track_id&#10;    positions_sorted = sorted(positions, key=lambda p: (p.frame_idx, p.track_id))&#10;    &#10;    return positions_sorted&#10;&#10;&#10;@app.post(&quot;/process-video&quot;, response_model=ProcessVideoResponse)&#10;async def process_video(&#10;    file: UploadFile = File(...),&#10;    annotations_json: str = Form(..., description=&quot;JSON string of pitch annotations for anchor frames&quot;)&#10;):&#10;    &quot;&quot;&quot;&#10;    Unified endpoint to process a video through the entire pipeline.&#10;    &#10;    Steps:&#10;    1. Upload video&#10;    2. Run YOLOv8n + ByteTrack tracking&#10;    3. Compute homographies at anchor frames&#10;    4. Map players to pitch coordinates&#10;    5. Interpolate player trajectories&#10;    6. Return JSON pitch coordinates&#10;    &#10;    Args:&#10;        file: MP4 video file&#10;        annotations_json: JSON string of pitch annotations for anchor frames&#10;&#10;    Returns:&#10;        ProcessVideoResponse with video_id, status, and player_positions&#10;    &quot;&quot;&quot;&#10;    # Read and validate file&#10;    content = await file.read()&#10;    validate_video_upload(file, content)&#10;&#10;    # Step 1: Upload video&#10;    video_id = str(uuid.uuid4())&#10;    video_path = VIDEOS_DIR / f&quot;{video_id}.mp4&quot;&#10;    &#10;    try:&#10;        with open(video_path, &quot;wb&quot;) as f:&#10;            f.write(content)&#10;        &#10;        # Extract metadata&#10;        metadata = get_video_metadata(str(video_path))&#10;        videos[video_id] = {&#10;            &quot;path&quot;: str(video_path),&#10;            &quot;fps&quot;: metadata[&quot;fps&quot;],&#10;            &quot;num_frames&quot;: metadata[&quot;num_frames&quot;]&#10;        }&#10;        logger.info(f&quot;Processing video {video_id}: {metadata['num_frames']} frames&quot;)&#10;    except Exception as e:&#10;        if video_path.exists():&#10;            video_path.unlink()&#10;        logger.error(f&quot;Failed to upload video: {e}&quot;)&#10;        raise HTTPException(status_code=400, detail=&quot;Failed to upload video. Ensure it is a valid MP4 file.&quot;)&#10;&#10;    try:&#10;        # Step 2: Run YOLOv8n + ByteTrack tracking&#10;        from pipeline.detect import run_tracking  # Import here to avoid top-level ML imports&#10;        logger.info(f&quot;Running tracking on video {video_id}&quot;)&#10;        detections = run_tracking(str(video_path))&#10;        if not detections:&#10;            raise HTTPException(status_code=500, detail=&quot;No detections found in video&quot;)&#10;&#10;        detections_cache[video_id] = detections&#10;        save_detections(video_id, detections)&#10;        &#10;        # Step 3: Compute homographies at anchor frames&#10;        # Parse annotations JSON&#10;        try:&#10;            annotations_list = json.loads(annotations_json)&#10;            annotations = [PitchAnnotation(**ann) for ann in annotations_list]&#10;        except Exception as e:&#10;            raise HTTPException(status_code=400, detail=&quot;Invalid annotations format. Expected JSON array of PitchAnnotation objects.&quot;)&#10;&#10;        annotations_dict = {}&#10;        for ann in annotations:&#10;            annotations_dict[ann.frame_idx] = ann.points&#10;        &#10;        homographies = compute_homographies_from_annotations(annotations_dict)&#10;        if not homographies:&#10;            raise HTTPException(status_code=400, detail=&quot;No valid homographies computed. Need at least 4 points per frame.&quot;)&#10;        &#10;        homographies_cache[video_id] = homographies&#10;        save_homographies(video_id, homographies)&#10;        &#10;        # Step 4: Map players to pitch coordinates&#10;        positions = map_players_to_pitch(detections, homographies)&#10;        if not positions:&#10;            raise HTTPException(status_code=500, detail=&quot;Failed to map players to pitch&quot;)&#10;        &#10;        player_positions_cache[video_id] = positions&#10;        &#10;        # Step 5: Interpolate trajectories&#10;        # Get frame range from video metadata&#10;        num_frames = metadata[&quot;num_frames&quot;]&#10;        interpolated = interpolate_trajectories(positions, 0, num_frames - 1)&#10;        &#10;        # Combine sparse and interpolated positions&#10;        all_positions = positions + [&#10;            p for p in interpolated if p.source == &quot;interpolated&quot;&#10;        ]&#10;        &#10;        # Sort by frame_idx, then track_id&#10;        all_positions_sorted = sorted(all_positions, key=lambda p: (p.frame_idx, p.track_id))&#10;        player_positions_cache[video_id] = all_positions_sorted&#10;        &#10;        logger.info(f&quot;Processing complete for video {video_id}: {len(all_positions_sorted)} positions&quot;)&#10;&#10;        return ProcessVideoResponse(&#10;            video_id=video_id,&#10;            status=&quot;completed&quot;,&#10;            player_positions=all_positions_sorted&#10;        )&#10;        &#10;    except HTTPException:&#10;        raise&#10;    except Exception as e:&#10;        # Cleanup on error&#10;        if video_path.exists():&#10;            video_path.unlink()&#10;        logger.error(f&quot;Processing failed for video {video_id}: {e}&quot;)&#10;        raise HTTPException(status_code=500, detail=&quot;Processing failed. Please try again.&quot;)&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/interactive_analytics_system_backend/extras/README.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/interactive_analytics_system_backend/extras/README.md" />
              <option name="updatedContent" value="# Extras&#10;&#10;Optional components that are not part of the core API deployment.&#10;&#10;## streamlit_app.py&#10;&#10;An alternative Streamlit-based frontend for local development and testing.&#10;&#10;### Usage&#10;&#10;```bash&#10;# Install Streamlit&#10;pip install streamlit requests&#10;&#10;# Run the app (requires the API backend to be running)&#10;streamlit run extras/streamlit_app.py&#10;```&#10;&#10;The Streamlit app expects the API to be running at `http://localhost:8000`.&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/interactive_analytics_system_backend/pipeline/__init__.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/interactive_analytics_system_backend/pipeline/__init__.py" />
              <option name="originalContent" value="&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;GAA Video Analysis Pipeline&#10;&#10;This package provides the core video analysis functionality:&#10;- detect: YOLO + ByteTrack player detection and tracking&#10;- homography: Pitch calibration and coordinate mapping&#10;- map_players: Map pixel coordinates to pitch coordinates&#10;- trajectories: Interpolate player trajectories between anchor frames&#10;- schemas: Pydantic models for API request/response validation&#10;- config: Configuration constants&#10;- gaa_pitch_config: GAA pitch vertex definitions&#10;- video: Video metadata utilities&#10;&quot;&quot;&quot;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/interactive_analytics_system_backend/pipeline/detect.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/interactive_analytics_system_backend/pipeline/detect.py" />
              <option name="originalContent" value="&quot;&quot;&quot;YOLO + ByteTrack detection and tracking.&quot;&quot;&quot;&#10;from typing import List&#10;import numpy as np&#10;from ultralytics import YOLO&#10;&#10;from pipeline.schemas import Detection&#10;from pipeline.config import YOLO_MODEL_PATH, DEFAULT_CONF&#10;&#10;&#10;def run_tracking(&#10;    video_path: str,&#10;    model_path: str = None,&#10;    conf: float = None&#10;) -&gt; List[Detection]:&#10;    &quot;&quot;&quot;&#10;    Run YOLO detection and ByteTrack tracking on a video.&#10;    &#10;    Args:&#10;        video_path: Path to input video file&#10;        model_path: Path to YOLO model weights (defaults to YOLO_MODEL_PATH)&#10;        conf: Confidence threshold (defaults to DEFAULT_CONF)&#10;    &#10;    Returns:&#10;        List of Detection objects with frame_idx, track_id, bbox, confidence&#10;    &quot;&quot;&quot;&#10;    if model_path is None:&#10;        model_path = YOLO_MODEL_PATH&#10;    if conf is None:&#10;        conf = DEFAULT_CONF&#10;    &#10;    model = YOLO(model_path)&#10;    &#10;    # Force CPU usage for Render free tier&#10;    results = model.track(&#10;        source=video_path,&#10;        tracker=&quot;bytetrack.yaml&quot;,&#10;        conf=conf,&#10;        save=False,&#10;        stream=False,&#10;        device=&quot;cpu&quot;  # Explicitly use CPU&#10;    )&#10;    &#10;    detections = []&#10;    &#10;    for frame_idx, r in enumerate(results):&#10;        if r.boxes is None:&#10;            continue&#10;        &#10;        boxes = r.boxes.xyxy.cpu().numpy()&#10;        confs = r.boxes.conf.cpu().numpy()&#10;        ids = r.boxes.id.cpu().numpy()&#10;        &#10;        for box, conf_score, tid in zip(boxes, confs, ids):&#10;            x1, y1, x2, y2 = box&#10;            detections.append(Detection(&#10;                frame_idx=frame_idx,&#10;                track_id=int(tid),&#10;                x1=float(x1),&#10;                y1=float(y1),&#10;                x2=float(x2),&#10;                y2=float(y2),&#10;                confidence=float(conf_score)&#10;            ))&#10;    &#10;    return detections&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;YOLO + ByteTrack detection and tracking.&quot;&quot;&quot;&#10;from typing import List&#10;import numpy as np&#10;&#10;from pipeline.schemas import Detection&#10;from pipeline.config import YOLO_MODEL_PATH, DEFAULT_CONF&#10;&#10;&#10;def run_tracking(&#10;    video_path: str,&#10;    model_path: str = None,&#10;    conf: float = None&#10;) -&gt; List[Detection]:&#10;    &quot;&quot;&quot;&#10;    Run YOLO detection and ByteTrack tracking on a video.&#10;    &#10;    Args:&#10;        video_path: Path to input video file&#10;        model_path: Path to YOLO model weights (defaults to YOLO_MODEL_PATH)&#10;        conf: Confidence threshold (defaults to DEFAULT_CONF)&#10;    &#10;    Returns:&#10;        List of Detection objects with frame_idx, track_id, bbox, confidence&#10;    &quot;&quot;&quot;&#10;    if model_path is None:&#10;        model_path = YOLO_MODEL_PATH&#10;    if conf is None:&#10;        conf = DEFAULT_CONF&#10;&#10;    # Import YOLO lazily to avoid heavy dependency at module import time&#10;    try:&#10;        from ultralytics import YOLO&#10;    except Exception as e:&#10;        raise RuntimeError(&#10;            &quot;ultralytics is required to run tracking. Install ultralytics or mock run_tracking in tests.&quot;&#10;        ) from e&#10;&#10;    model = YOLO(model_path)&#10;    &#10;    # Force CPU usage for Render free tier&#10;    results = model.track(&#10;        source=video_path,&#10;        tracker=&quot;bytetrack.yaml&quot;,&#10;        conf=conf,&#10;        save=False,&#10;        stream=False,&#10;        device=&quot;cpu&quot;  # Explicitly use CPU&#10;    )&#10;    &#10;    detections = []&#10;    &#10;    for frame_idx, r in enumerate(results):&#10;        if r.boxes is None:&#10;            continue&#10;        &#10;        boxes = r.boxes.xyxy.cpu().numpy()&#10;        confs = r.boxes.conf.cpu().numpy()&#10;        ids = r.boxes.id.cpu().numpy()&#10;        &#10;        for box, conf_score, tid in zip(boxes, confs, ids):&#10;            x1, y1, x2, y2 = box&#10;            detections.append(Detection(&#10;                frame_idx=frame_idx,&#10;                track_id=int(tid),&#10;                x1=float(x1),&#10;                y1=float(y1),&#10;                x2=float(x2),&#10;                y2=float(y2),&#10;                confidence=float(conf_score)&#10;            ))&#10;    &#10;    return detections" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/interactive_analytics_system_backend/pipeline/homography.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/interactive_analytics_system_backend/pipeline/homography.py" />
              <option name="originalContent" value="&quot;&quot;&quot;Homography computation and pixel-to-pitch mapping.&quot;&quot;&quot;&#10;from typing import Tuple, Dict, List&#10;import numpy as np&#10;import cv2&#10;&#10;from pipeline.config import PITCH_W, PITCH_H, OUT_W, OUT_H, K1&#10;from pipeline.schemas import PitchPoint&#10;from gaa_pitch_config import GAA_PITCH_VERTICES&#10;&#10;&#10;def compute_homography(&#10;    pts_image: np.ndarray,&#10;    pts_pitch_norm: np.ndarray&#10;) -&gt; np.ndarray:&#10;    &quot;&quot;&quot;&#10;    Compute homography matrix from image points to normalized pitch points.&#10;    &#10;    Args:&#10;        pts_image: Nx2 array of image coordinates (x_img, y_img)&#10;        pts_pitch_norm: Nx2 array of normalized pitch coordinates (x_pitch_norm, y_pitch_norm)&#10;    &#10;    Returns:&#10;        3x3 homography matrix&#10;    &quot;&quot;&quot;&#10;    H, _ = cv2.findHomography(&#10;        pts_image.astype(np.float32),&#10;        pts_pitch_norm.astype(np.float32),&#10;        cv2.RANSAC,&#10;        5.0&#10;    )&#10;    &#10;    if H is None:&#10;        raise ValueError(&quot;Failed to compute homography&quot;)&#10;    &#10;    return H&#10;&#10;&#10;def normalize_pitch_points(pts_pitch: np.ndarray) -&gt; np.ndarray:&#10;    &quot;&quot;&quot;&#10;    Normalize pitch points to output canvas dimensions.&#10;    &#10;    Args:&#10;        pts_pitch: Nx2 array of pitch coordinates in meters (x, y)&#10;    &#10;    Returns:&#10;        Nx2 array of normalized pitch coordinates in pixels&#10;    &quot;&quot;&quot;&#10;    pts_pitch_norm = np.column_stack([&#10;        pts_pitch[:, 0] / PITCH_W * OUT_W,&#10;        pts_pitch[:, 1] / PITCH_H * OUT_H&#10;    ]).astype(np.float32)&#10;    &#10;    return pts_pitch_norm&#10;&#10;&#10;def map_pixel_to_distorted_pitch(&#10;    x_img: float,&#10;    y_img: float,&#10;    H: np.ndarray,&#10;    out_w: int = None,&#10;    out_h: int = None,&#10;    k1: float = None&#10;) -&gt; Tuple[float, float]:&#10;    &quot;&quot;&quot;&#10;    Map a single image pixel to distorted pitch coordinates.&#10;    &#10;    Uses homography + radial distortion model from the notebook.&#10;    &#10;    Args:&#10;        x_img: Image x coordinate&#10;        y_img: Image y coordinate&#10;        H: 3x3 homography matrix&#10;        out_w: Output canvas width (defaults to OUT_W)&#10;        out_h: Output canvas height (defaults to OUT_H)&#10;        k1: Radial distortion coefficient (defaults to K1)&#10;    &#10;    Returns:&#10;        Tuple of (x_pitch, y_pitch) in normalized pitch space&#10;    &quot;&quot;&quot;&#10;    if out_w is None:&#10;        out_w = OUT_W&#10;    if out_h is None:&#10;        out_h = OUT_H&#10;    if k1 is None:&#10;        k1 = K1&#10;    &#10;    H_inv = np.linalg.inv(H)&#10;    &#10;    # Project to pitch plane (ideal)&#10;    p = np.array([x_img, y_img, 1.0], dtype=np.float32)&#10;    pitch = H @ p&#10;    pitch /= pitch[2]&#10;    &#10;    x, y = pitch[0], pitch[1]&#10;    &#10;    # Apply radial distortion in pitch space&#10;    cx, cy = out_w / 2, out_h / 2&#10;    dx = x - cx&#10;    dy = y - cy&#10;    r2 = dx * dx + dy * dy&#10;    &#10;    x_d = x + dx * k1 * r2&#10;    y_d = y + dy * k1 * r2&#10;    &#10;    return x_d, y_d&#10;&#10;&#10;def compute_homographies_from_annotations(&#10;    annotations: Dict[int, List[PitchPoint]]&#10;) -&gt; Dict[int, np.ndarray]:&#10;    &quot;&quot;&quot;&#10;    Compute homography matrices for multiple frames from pitch annotations.&#10;    &#10;    Args:&#10;        annotations: Dict mapping frame_idx to list of (pitch_id, x_img, y_img) tuples&#10;    &#10;    Returns:&#10;        Dict mapping frame_idx to 3x3 homography matrix&#10;    &quot;&quot;&quot;&#10;    homographies = {}&#10;    &#10;    for frame_idx, points in annotations.items():&#10;        if len(points) &lt; 4:&#10;            continue&#10;        &#10;        # Extract image points&#10;        pts_image = np.array([&#10;            [p.x_img, p.y_img] for p in points&#10;        ], dtype=np.float32)&#10;        &#10;        # Extract pitch points from pitch_id&#10;        pts_pitch = np.array([&#10;            GAA_PITCH_VERTICES[p.pitch_id] for p in points&#10;        ], dtype=np.float32)&#10;        &#10;        # Normalize pitch points&#10;        pts_pitch_norm = normalize_pitch_points(pts_pitch)&#10;        &#10;        # Compute homography&#10;        try:&#10;            H = compute_homography(pts_image, pts_pitch_norm)&#10;            homographies[frame_idx] = H&#10;        except ValueError:&#10;            continue&#10;    &#10;    return homographies&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;Homography computation and pixel-to-pitch mapping.&quot;&quot;&quot;&#10;from typing import Tuple, Dict, List&#10;import numpy as np&#10;import cv2&#10;&#10;from pipeline.config import PITCH_W, PITCH_H, OUT_W, OUT_H, K1&#10;from pipeline.schemas import PitchPoint&#10;from pipeline.gaa_pitch_config import GAA_PITCH_VERTICES&#10;&#10;&#10;def compute_homography(&#10;    pts_image: np.ndarray,&#10;    pts_pitch_norm: np.ndarray&#10;) -&gt; np.ndarray:&#10;    &quot;&quot;&quot;&#10;    Compute homography matrix from image points to normalized pitch points.&#10;    &#10;    Args:&#10;        pts_image: Nx2 array of image coordinates (x_img, y_img)&#10;        pts_pitch_norm: Nx2 array of normalized pitch coordinates (x_pitch_norm, y_pitch_norm)&#10;    &#10;    Returns:&#10;        3x3 homography matrix&#10;    &quot;&quot;&quot;&#10;    H, _ = cv2.findHomography(&#10;        pts_image.astype(np.float32),&#10;        pts_pitch_norm.astype(np.float32),&#10;        cv2.RANSAC,&#10;        5.0&#10;    )&#10;    &#10;    if H is None:&#10;        raise ValueError(&quot;Failed to compute homography&quot;)&#10;    &#10;    return H&#10;&#10;&#10;def normalize_pitch_points(pts_pitch: np.ndarray) -&gt; np.ndarray:&#10;    &quot;&quot;&quot;&#10;    Normalize pitch points to output canvas dimensions.&#10;    &#10;    Args:&#10;        pts_pitch: Nx2 array of pitch coordinates in meters (x, y)&#10;    &#10;    Returns:&#10;        Nx2 array of normalized pitch coordinates in pixels&#10;    &quot;&quot;&quot;&#10;    pts_pitch_norm = np.column_stack([&#10;        pts_pitch[:, 0] / PITCH_W * OUT_W,&#10;        pts_pitch[:, 1] / PITCH_H * OUT_H&#10;    ]).astype(np.float32)&#10;    &#10;    return pts_pitch_norm&#10;&#10;&#10;def map_pixel_to_distorted_pitch(&#10;    x_img: float,&#10;    y_img: float,&#10;    H: np.ndarray,&#10;    out_w: int = None,&#10;    out_h: int = None,&#10;    k1: float = None&#10;) -&gt; Tuple[float, float]:&#10;    &quot;&quot;&quot;&#10;    Map a single image pixel to distorted pitch coordinates.&#10;    &#10;    Uses homography + radial distortion model from the notebook.&#10;    &#10;    Args:&#10;        x_img: Image x coordinate&#10;        y_img: Image y coordinate&#10;        H: 3x3 homography matrix&#10;        out_w: Output canvas width (defaults to OUT_W)&#10;        out_h: Output canvas height (defaults to OUT_H)&#10;        k1: Radial distortion coefficient (defaults to K1)&#10;    &#10;    Returns:&#10;        Tuple of (x_pitch, y_pitch) in normalized pitch space&#10;    &quot;&quot;&quot;&#10;    if out_w is None:&#10;        out_w = OUT_W&#10;    if out_h is None:&#10;        out_h = OUT_H&#10;    if k1 is None:&#10;        k1 = K1&#10;    &#10;    H_inv = np.linalg.inv(H)&#10;    &#10;    # Project to pitch plane (ideal)&#10;    p = np.array([x_img, y_img, 1.0], dtype=np.float32)&#10;    pitch = H @ p&#10;    pitch /= pitch[2]&#10;    &#10;    x, y = pitch[0], pitch[1]&#10;    &#10;    # Apply radial distortion in pitch space&#10;    cx, cy = out_w / 2, out_h / 2&#10;    dx = x - cx&#10;    dy = y - cy&#10;    r2 = dx * dx + dy * dy&#10;    &#10;    x_d = x + dx * k1 * r2&#10;    y_d = y + dy * k1 * r2&#10;    &#10;    return x_d, y_d&#10;&#10;&#10;def compute_homographies_from_annotations(&#10;    annotations: Dict[int, List[PitchPoint]]&#10;) -&gt; Dict[int, np.ndarray]:&#10;    &quot;&quot;&quot;&#10;    Compute homography matrices for multiple frames from pitch annotations.&#10;    &#10;    Args:&#10;        annotations: Dict mapping frame_idx to list of (pitch_id, x_img, y_img) tuples&#10;    &#10;    Returns:&#10;        Dict mapping frame_idx to 3x3 homography matrix&#10;    &quot;&quot;&quot;&#10;    homographies = {}&#10;    &#10;    for frame_idx, points in annotations.items():&#10;        if len(points) &lt; 4:&#10;            continue&#10;        &#10;        # Extract image points&#10;        pts_image = np.array([&#10;            [p.x_img, p.y_img] for p in points&#10;        ], dtype=np.float32)&#10;        &#10;        # Extract pitch points from pitch_id&#10;        pts_pitch = np.array([&#10;            GAA_PITCH_VERTICES[p.pitch_id] for p in points&#10;        ], dtype=np.float32)&#10;        &#10;        # Normalize pitch points&#10;        pts_pitch_norm = normalize_pitch_points(pts_pitch)&#10;        &#10;        # Compute homography&#10;        try:&#10;            H = compute_homography(pts_image, pts_pitch_norm)&#10;            homographies[frame_idx] = H&#10;        except ValueError:&#10;            continue&#10;    &#10;    return homographies&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/interactive_analytics_system_backend/pyproject.toml">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/interactive_analytics_system_backend/pyproject.toml" />
              <option name="updatedContent" value="[build-system]&#10;requires = [&quot;setuptools&gt;=61.0&quot;]&#10;build-backend = &quot;setuptools.build_meta&quot;&#10;&#10;[project]&#10;name = &quot;gaa-video-analysis-api&quot;&#10;version = &quot;1.0.0&quot;&#10;description = &quot;FastAPI backend for GAA video analysis with player tracking&quot;&#10;readme = &quot;README.md&quot;&#10;requires-python = &quot;&gt;=3.10&quot;&#10;license = {text = &quot;MIT&quot;}&#10;authors = [&#10;    {name = &quot;GAA Video Analysis Team&quot;}&#10;]&#10;keywords = [&quot;gaa&quot;, &quot;video-analysis&quot;, &quot;player-tracking&quot;, &quot;fastapi&quot;, &quot;yolo&quot;]&#10;classifiers = [&#10;    &quot;Development Status :: 4 - Beta&quot;,&#10;    &quot;Framework :: FastAPI&quot;,&#10;    &quot;Intended Audience :: Developers&quot;,&#10;    &quot;License :: OSI Approved :: MIT License&quot;,&#10;    &quot;Programming Language :: Python :: 3&quot;,&#10;    &quot;Programming Language :: Python :: 3.10&quot;,&#10;    &quot;Programming Language :: Python :: 3.11&quot;,&#10;    &quot;Programming Language :: Python :: 3.12&quot;,&#10;]&#10;&#10;[project.urls]&#10;&quot;Homepage&quot; = &quot;https://github.com/your-username/GAA-Video-Analysis&quot;&#10;&quot;Bug Tracker&quot; = &quot;https://github.com/your-username/GAA-Video-Analysis/issues&quot;&#10;&#10;[tool.pytest.ini_options]&#10;testpaths = [&quot;tests&quot;]&#10;python_files = [&quot;test_*.py&quot;]&#10;python_functions = [&quot;test_*&quot;]&#10;addopts = &quot;-v --tb=short&quot;&#10;filterwarnings = [&#10;    &quot;ignore::DeprecationWarning&quot;,&#10;]&#10;&#10;[tool.coverage.run]&#10;source = [&quot;pipeline&quot;, &quot;app&quot;]&#10;omit = [&quot;tests/*&quot;, &quot;*/__pycache__/*&quot;]&#10;&#10;[tool.coverage.report]&#10;exclude_lines = [&#10;    &quot;pragma: no cover&quot;,&#10;    &quot;def __repr__&quot;,&#10;    &quot;raise NotImplementedError&quot;,&#10;    &quot;if __name__ == .__main__.:&quot;,&#10;]&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/interactive_analytics_system_backend/render.yaml">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/interactive_analytics_system_backend/render.yaml" />
              <option name="originalContent" value="services:&#10;  - type: web&#10;    name: gaa-video-analysis-backend&#10;    env: python&#10;    buildCommand: pip install -r requirements.txt&#10;    startCommand: uvicorn app:app --host 0.0.0.0 --port $PORT&#10;    envVars:&#10;      - key: YOLO_MODEL_PATH&#10;        value: yolov8n.pt&#10;      - key: PORT&#10;        value: 8000&#10;" />
              <option name="updatedContent" value="services:&#10;  - type: web&#10;    name: gaa-video-analysis-api&#10;    env: python&#10;    plan: free&#10;    buildCommand: pip install -r requirements.txt&#10;    startCommand: uvicorn app:app --host 0.0.0.0 --port $PORT&#10;    healthCheckPath: /health&#10;    envVars:&#10;      - key: PYTHON_VERSION&#10;        value: &quot;3.11&quot;&#10;      - key: YOLO_MODEL_PATH&#10;        value: models/best.pt&#10;      - key: ALLOWED_ORIGINS&#10;        sync: false  # Set this in Render dashboard to your frontend URL&#10;      - key: MAX_VIDEO_SIZE_MB&#10;        value: &quot;100&quot;  # Reduced for free tier memory limits&#10;      - key: DATA_DIR&#10;        value: data" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/interactive_analytics_system_backend/requirements-dev.txt">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/interactive_analytics_system_backend/requirements-dev.txt" />
              <option name="updatedContent" value="# Development and testing dependencies&#10;pytest&gt;=7.0.0&#10;pytest-cov&gt;=4.0.0&#10;httpx&gt;=0.24.0&#10;python-multipart&gt;=0.0.6&#10;&#10;# Already in requirements.txt but needed for tests&#10;fastapi&gt;=0.104.0&#10;starlette&gt;=0.27.0&#10;pydantic&gt;=2.0.0&#10;numpy&gt;=1.24.0&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/interactive_analytics_system_backend/requirements.txt">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/interactive_analytics_system_backend/requirements.txt" />
              <option name="originalContent" value="# Core&#10;numpy&gt;=1.24.0&#10;pandas&gt;=2.0.0&#10;opencv-python&gt;=4.8.0&#10;&#10;# YOLO + ByteTrack (CPU-only)&#10;ultralytics&gt;=8.0.0&#10;torch&gt;=2.0.0&#10;torchvision&gt;=0.15.0&#10;&#10;# API backend&#10;fastapi&gt;=0.104.0&#10;uvicorn[standard]&gt;=0.24.0&#10;&#10;# Data &amp; serialization&#10;pydantic&gt;=2.0.0&#10;&#10;# Video handling&#10;imageio&gt;=2.31.0&#10;imageio-ffmpeg&gt;=0.4.9&#10;" />
              <option name="updatedContent" value="# Core&#10;numpy&gt;=1.24.0&#10;pandas&gt;=2.0.0&#10;opencv-python-headless&gt;=4.8.0  # Headless for server deployment&#10;&#10;# YOLO + ByteTrack (CPU-only)&#10;ultralytics&gt;=8.0.0&#10;torch&gt;=2.0.0&#10;torchvision&gt;=0.15.0&#10;&#10;# API backend&#10;fastapi&gt;=0.104.0&#10;uvicorn[standard]&gt;=0.24.0&#10;python-multipart&gt;=0.0.6  # Required for file uploads&#10;&#10;# Data &amp; serialization&#10;pydantic&gt;=2.0.0&#10;&#10;# Video handling&#10;imageio&gt;=2.31.0&#10;imageio-ffmpeg&gt;=0.4.9" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/interactive_analytics_system_backend/tests/__init__.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/interactive_analytics_system_backend/tests/__init__.py" />
              <option name="updatedContent" value="&quot;&quot;&quot;Test suite for GAA Video Analysis API.&quot;&quot;&quot;&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/interactive_analytics_system_backend/tests/conftest.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/interactive_analytics_system_backend/tests/conftest.py" />
              <option name="originalContent" value="&#10;&#10;&#10;&#10;" />
              <option name="updatedContent" value="from pathlib import Path&#10;import numpy as np&#10;import pytest&#10;from fastapi.testclient import TestClient&#10;&#10;# Ensure project root is on sys.path so `pipeline` imports work when tests run&#10;import sys&#10;sys.path.insert(0, str(Path(__file__).resolve().parents[1]))&#10;&#10;# Provide a lightweight fake `ultralytics` module during tests to avoid requiring heavy ML deps&#10;import types&#10;if &quot;ultralytics&quot; not in sys.modules:&#10;    ultralytics_stub = types.ModuleType(&quot;ultralytics&quot;)&#10;    class YOLO:&#10;        def __init__(self, *args, **kwargs):&#10;            pass&#10;        def track(self, *args, **kwargs):&#10;            return []&#10;    ultralytics_stub.YOLO = YOLO&#10;    sys.modules[&quot;ultralytics&quot;] = ultralytics_stub&#10;&#10;from pipeline.schemas import Detection, PitchAnnotation, PitchPoint, PlayerPitchPosition&#10;from app import app, VIDEOS_DIR, TRACKS_DIR, ANNOTATIONS_DIR&#10;&#10;&#10;@pytest.fixture(autouse=True)&#10;def use_temp_dirs(tmp_path, monkeypatch):&#10;    &quot;&quot;&quot;Redirect data directories to a temporary path for tests.&quot;&quot;&quot;&#10;    temp_data = tmp_path / &quot;data&quot;&#10;    temp_videos = temp_data / &quot;videos&quot;&#10;    temp_tracks = temp_data / &quot;tracks&quot;&#10;    temp_annotations = temp_data / &quot;annotations&quot;&#10;    temp_videos.mkdir(parents=True)&#10;    temp_tracks.mkdir(parents=True)&#10;    temp_annotations.mkdir(parents=True)&#10;&#10;    monkeypatch.setattr(&quot;app.VIDEOS_DIR&quot;, temp_videos)&#10;    monkeypatch.setattr(&quot;app.TRACKS_DIR&quot;, temp_tracks)&#10;    monkeypatch.setattr(&quot;app.ANNOTATIONS_DIR&quot;, temp_annotations)&#10;&#10;    yield&#10;&#10;&#10;@pytest.fixture&#10;def client():&#10;    return TestClient(app)&#10;&#10;&#10;@pytest.fixture&#10;def sample_detections():&#10;    # Two tracks across frames 0..3&#10;    dets = [&#10;        Detection(frame_idx=0, track_id=1, x1=100, y1=100, x2=150, y2=200, confidence=0.9),&#10;        Detection(frame_idx=2, track_id=1, x1=105, y1=98, x2=155, y2=198, confidence=0.88),&#10;        Detection(frame_idx=0, track_id=2, x1=300, y1=120, x2=340, y2=220, confidence=0.85),&#10;        Detection(frame_idx=3, track_id=2, x1=305, y1=125, x2=345, y2=225, confidence=0.86),&#10;    ]&#10;    return dets&#10;&#10;&#10;@pytest.fixture&#10;def sample_annotations():&#10;    # Simple square pitch points that map to normalized corners&#10;    annotations = [&#10;        {&#10;            &quot;frame_idx&quot;: 0,&#10;            &quot;points&quot;: [&#10;                {&quot;pitch_id&quot;: &quot;corner_tl&quot;, &quot;x_img&quot;: 0, &quot;y_img&quot;: 0},&#10;                {&quot;pitch_id&quot;: &quot;corner_tr&quot;, &quot;x_img&quot;: 400, &quot;y_img&quot;: 0},&#10;                {&quot;pitch_id&quot;: &quot;corner_bl&quot;, &quot;x_img&quot;: 0, &quot;y_img&quot;: 800},&#10;                {&quot;pitch_id&quot;: &quot;corner_br&quot;, &quot;x_img&quot;: 400, &quot;y_img&quot;: 800},&#10;            ]&#10;        }&#10;    ]&#10;    # Build PitchAnnotation objects for programmatic uses&#10;    ann_objs = [PitchAnnotation(**a) for a in annotations]&#10;    return ann_objs&#10;&#10;&#10;@pytest.fixture&#10;def bad_annotations():&#10;    # Less than 4 points&#10;    annotations = [&#10;        {&#10;            &quot;frame_idx&quot;: 0,&#10;            &quot;points&quot;: [&#10;                {&quot;pitch_id&quot;: &quot;corner_tl&quot;, &quot;x_img&quot;: 0, &quot;y_img&quot;: 0},&#10;                {&quot;pitch_id&quot;: &quot;corner_tr&quot;, &quot;x_img&quot;: 400, &quot;y_img&quot;: 0},&#10;            ]&#10;        }&#10;    ]&#10;    return [PitchAnnotation(**a) for a in annotations]&#10;&#10;&#10;@pytest.fixture&#10;def sample_homography():&#10;    # Identity-like homography mapping image coords directly to pitch-normalized space&#10;    H = np.eye(3, dtype=np.float32)&#10;    return {0: H}&#10;&#10;&#10;@pytest.fixture&#10;def sample_positions(sample_detections, sample_homography):&#10;    # Map bottom-center manually using identity H: bottom center = (x_center, y_bottom)&#10;    positions = []&#10;    for d in sample_detections:&#10;        x_center = (d.x1 + d.x2) / 2&#10;        y_bottom = d.y2&#10;        positions.append(PlayerPitchPosition(&#10;            frame_idx=d.frame_idx,&#10;            track_id=d.track_id,&#10;            x_pitch=float(x_center),&#10;            y_pitch=float(y_bottom),&#10;            source=&quot;homography&quot;&#10;        ))&#10;    return positions&#10;&#10;&#10;@pytest.fixture&#10;def sample_video_metadata(monkeypatch):&#10;    def fake_metadata(path):&#10;        return {&quot;fps&quot;: 25, &quot;num_frames&quot;: 10}&#10;    # Patch both pipeline.video.get_video_metadata and the name imported into app&#10;    monkeypatch.setattr(&quot;pipeline.video.get_video_metadata&quot;, fake_metadata)&#10;    monkeypatch.setattr(&quot;app.get_video_metadata&quot;, fake_metadata)&#10;    return fake_metadata&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/interactive_analytics_system_backend/tests/test_endpoints.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/interactive_analytics_system_backend/tests/test_endpoints.py" />
              <option name="originalContent" value="&#10;&#10;&#10;" />
              <option name="updatedContent" value="import json&#10;from io import BytesIO&#10;&#10;&#10;def test_upload_video_and_track(client, sample_video_metadata, monkeypatch, sample_detections):&#10;    # Monkeypatch run_tracking to return sample detections&#10;    def fake_run_tracking(path):&#10;        return sample_detections&#10;    monkeypatch.setattr(&quot;pipeline.detect.run_tracking&quot;, fake_run_tracking)&#10;    # `app.run_tracking` may not exist because the import is lazy; allow setting it without raising&#10;    monkeypatch.setattr(&quot;app.run_tracking&quot;, fake_run_tracking, raising=False)&#10;&#10;    # Upload a fake video file&#10;    fake_file = BytesIO(b&quot;fake mp4 data&quot;)&#10;    response = client.post(&#10;        &quot;/videos&quot;,&#10;        files={&quot;file&quot;: (&quot;test.mp4&quot;, fake_file, &quot;video/mp4&quot;)}&#10;    )&#10;    assert response.status_code == 200&#10;    body = response.json()&#10;    video_id = body[&quot;video_id&quot;]&#10;&#10;    # Run tracking&#10;    resp2 = client.post(f&quot;/videos/{video_id}/track&quot;)&#10;    assert resp2.status_code == 200&#10;    body2 = resp2.json()&#10;    assert body2[&quot;frames_processed&quot;] &gt;= 1&#10;    assert body2[&quot;tracks&quot;] &gt;= 1&#10;&#10;&#10;def test_homographies_endpoint(client, sample_video_metadata, sample_annotations):&#10;    # Upload a fake video first&#10;    fake_file = BytesIO(b&quot;fake mp4 data&quot;)&#10;    r = client.post(&quot;/videos&quot;, files={&quot;file&quot;: (&quot;v.mp4&quot;, fake_file, &quot;video/mp4&quot;)})&#10;    assert r.status_code == 200&#10;    vid = r.json()[&quot;video_id&quot;]&#10;&#10;    # Compute homographies with good annotations&#10;    payload = [a.dict() for a in sample_annotations]&#10;    resp = client.post(f&quot;/videos/{vid}/homographies&quot;, json=payload)&#10;    assert resp.status_code == 200&#10;    frames = resp.json()[&quot;frames&quot;]&#10;    assert 0 in frames&#10;&#10;&#10;def test_homographies_bad_annotations(client, sample_video_metadata, bad_annotations):&#10;    fake_file = BytesIO(b&quot;fake mp4 data&quot;)&#10;    r = client.post(&quot;/videos&quot;, files={&quot;file&quot;: (&quot;v.mp4&quot;, fake_file, &quot;video/mp4&quot;)})&#10;    vid = r.json()[&quot;video_id&quot;]&#10;&#10;    payload = [a.dict() for a in bad_annotations]&#10;    resp = client.post(f&quot;/videos/{vid}/homographies&quot;, json=payload)&#10;    assert resp.status_code == 400&#10;&#10;&#10;def test_map_players_and_interpolate_full_flow(client, monkeypatch, sample_video_metadata, sample_detections, sample_annotations, sample_homography, sample_positions):&#10;    # Patch run_tracking&#10;    monkeypatch.setattr(&quot;pipeline.detect.run_tracking&quot;, lambda path: sample_detections)&#10;    # Upload video&#10;    fake_file = BytesIO(b&quot;fake mp4 data&quot;)&#10;    r = client.post(&quot;/videos&quot;, files={&quot;file&quot;: (&quot;v.mp4&quot;, fake_file, &quot;video/mp4&quot;)})&#10;    vid = r.json()[&quot;video_id&quot;]&#10;&#10;    # Run tracking&#10;    client.post(f&quot;/videos/{vid}/track&quot;)&#10;&#10;    # Compute homographies (patch compute function to return our sample homography)&#10;    monkeypatch.setattr(&quot;pipeline.homography.compute_homographies_from_annotations&quot;, lambda ann: sample_homography)&#10;    payload = [a.dict() for a in sample_annotations]&#10;    client.post(f&quot;/videos/{vid}/homographies&quot;, json=payload)&#10;&#10;    # Map players&#10;    resp = client.post(f&quot;/videos/{vid}/map_players&quot;)&#10;    assert resp.status_code == 200&#10;    positions = resp.json()&#10;    assert len(positions) &gt;= 1&#10;&#10;    # Patch interpolate to return a small interpolated list (simulate behavior)&#10;    def fake_interpolate(positions, start, end):&#10;        # Return one interpolated position for frame 1&#10;        return [&#10;            {&#10;                &quot;frame_idx&quot;: 1,&#10;                &quot;track_id&quot;: positions[0][&quot;track_id&quot;] if isinstance(positions[0], dict) else positions[0].track_id,&#10;                &quot;x_pitch&quot;: 123.0,&#10;                &quot;y_pitch&quot;: 456.0,&#10;                &quot;source&quot;: &quot;interpolated&quot;&#10;            }&#10;        ]&#10;&#10;    monkeypatch.setattr(&quot;pipeline.trajectories.interpolate_trajectories&quot;, fake_interpolate)&#10;&#10;    resp2 = client.post(f&quot;/videos/{vid}/interpolate?start_frame=0&amp;end_frame=5&quot;)&#10;    assert resp2.status_code == 200&#10;    body = resp2.json()&#10;    assert body[&quot;method&quot;] == &quot;linear&quot;&#10;&#10;&#10;def test_process_video_full_pipeline(client, monkeypatch, sample_annotations, sample_homography, sample_detections, sample_positions, sample_video_metadata):&#10;    # Patch heavy functions&#10;    monkeypatch.setattr(&quot;pipeline.detect.run_tracking&quot;, lambda path: sample_detections)&#10;    monkeypatch.setattr(&quot;pipeline.homography.compute_homographies_from_annotations&quot;, lambda ann: sample_homography)&#10;    monkeypatch.setattr(&quot;pipeline.map_players.map_players_to_pitch&quot;, lambda dets, homogs: sample_positions)&#10;    monkeypatch.setattr(&quot;pipeline.trajectories.interpolate_trajectories&quot;, lambda pos, s, e: [])&#10;&#10;    annotations_json = json.dumps([a.model_dump() for a in sample_annotations])&#10;    fake_file = BytesIO(b&quot;fake mp4 data&quot;)&#10;    resp = client.post(&#10;        &quot;/process-video&quot;,&#10;        data={&quot;annotations_json&quot;: annotations_json},&#10;        files={&quot;file&quot;: (&quot;v.mp4&quot;, fake_file, &quot;video/mp4&quot;)}&#10;    )&#10;    assert resp.status_code == 200&#10;    body = resp.json()&#10;    assert body[&quot;status&quot;] == &quot;completed&quot;&#10;    assert &quot;player_positions&quot; in body&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/interactive_analytics_system_backend/tests/test_homography.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/interactive_analytics_system_backend/tests/test_homography.py" />
              <option name="originalContent" value="    assert abs(y - 50.0) &lt; 1e-6&#10;    assert abs(x - 100.0) &lt; 1e-6&#10;    # With identity H and k1=0, expect same x,y&#10;    x, y = map_pixel_to_distorted_pitch(100.0, 50.0, H, out_w=400, out_h=800, k1=0.0)&#10;    H = sample_homography[0]&#10;def test_map_pixel_identity_homography(sample_homography):&#10;&#10;&#10;    assert H.shape == (3, 3)&#10;    H = homogs[0]&#10;    assert 0 in homogs&#10;    assert isinstance(homogs, dict)&#10;    homogs = compute_homographies_from_annotations(annotations_dict)&#10;    annotations_dict = {ann.frame_idx: ann.points for ann in sample_annotations}&#10;def test_compute_homographies_with_valid_annotations(sample_annotations):&#10;&#10;&#10;from pipeline.homography import compute_homographies_from_annotations, map_pixel_to_distorted_pitch&#10;&#10;" />
              <option name="updatedContent" value="from pipeline.homography import compute_homographies_from_annotations, map_pixel_to_distorted_pitch&#10;import numpy as np&#10;&#10;&#10;def test_compute_homographies_with_valid_annotations(sample_annotations):&#10;    annotations_dict = {ann.frame_idx: ann.points for ann in sample_annotations}&#10;    homogs = compute_homographies_from_annotations(annotations_dict)&#10;    assert isinstance(homogs, dict)&#10;    assert 0 in homogs&#10;    H = homogs[0]&#10;    assert H.shape == (3, 3)&#10;&#10;&#10;def test_map_pixel_identity_homography(sample_homography):&#10;    H = sample_homography[0]&#10;    x, y = map_pixel_to_distorted_pitch(100.0, 50.0, H, out_w=400, out_h=800, k1=0.0)&#10;    # With identity H and k1=0, expect same x,y&#10;    assert abs(x - 100.0) &lt; 1e-6&#10;    assert abs(y - 50.0) &lt; 1e-6" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/interactive_analytics_system_backend/tests/test_map_players.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/interactive_analytics_system_backend/tests/test_map_players.py" />
              <option name="updatedContent" value="from pipeline.map_players import map_players_to_pitch&#10;from pipeline.schemas import Detection&#10;&#10;&#10;def test_map_players_bottom_center(sample_detections, sample_homography):&#10;    # Use the sample detections and homography (identity)&#10;    positions = map_players_to_pitch(sample_detections, sample_homography, out_w=400, out_h=800, k1=0.0)&#10;    # Should map entries only for frames that have homography (frame 0 for our sample_homography)&#10;    assert all(p.source == &quot;homography&quot; for p in positions)&#10;    # For frame 0, there are two detections (track 1 and 2)&#10;    frame0_positions = [p for p in positions if p.frame_idx == 0]&#10;    assert len(frame0_positions) == 2&#10;    # Check bottom-center calculation for first detection&#10;    first_det = next(d for d in sample_detections if d.frame_idx == 0 and d.track_id == 1)&#10;    expected_x = (first_det.x1 + first_det.x2) / 2&#10;    expected_y = first_det.y2&#10;    p = next(p for p in frame0_positions if p.track_id == 1)&#10;    assert abs(p.x_pitch - expected_x) &lt; 1e-6&#10;    assert abs(p.y_pitch - expected_y) &lt; 1e-6&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/interactive_analytics_system_backend/tests/test_trajectories.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/interactive_analytics_system_backend/tests/test_trajectories.py" />
              <option name="updatedContent" value="from pipeline.trajectories import interpolate_trajectories&#10;from pipeline.schemas import PlayerPitchPosition&#10;&#10;&#10;def test_interpolate_linear_two_anchors():&#10;    sparse = [&#10;        PlayerPitchPosition(frame_idx=0, track_id=1, x_pitch=0.0, y_pitch=0.0, source=&quot;homography&quot;),&#10;        PlayerPitchPosition(frame_idx=10, track_id=1, x_pitch=10.0, y_pitch=10.0, source=&quot;homography&quot;)&#10;    ]&#10;    interpolated = interpolate_trajectories(sparse, 0, 10)&#10;    # Should contain frames 0..10 inclusive&#10;    frames = sorted(set(p.frame_idx for p in interpolated))&#10;    assert frames[0] == 0&#10;    assert frames[-1] == 10&#10;    # Check middle frame ~5&#10;    mid = next(p for p in interpolated if p.frame_idx == 5 and p.track_id == 1)&#10;    assert abs(mid.x_pitch - 5.0) &lt; 1e-6&#10;    assert abs(mid.y_pitch - 5.0) &lt; 1e-6&#10;    # Anchors preserved&#10;    start = next(p for p in interpolated if p.frame_idx == 0 and p.track_id == 1)&#10;    assert start.source == &quot;homography&quot;&#10;    end = next(p for p in interpolated if p.frame_idx == 10 and p.track_id == 1)&#10;    assert end.source == &quot;homography&quot;&#10;&#10;&#10;def test_interpolate_single_anchor_returns_anchor_only():&#10;    sparse = [&#10;        PlayerPitchPosition(frame_idx=4, track_id=2, x_pitch=50.0, y_pitch=60.0, source=&quot;homography&quot;)&#10;    ]&#10;    interpolated = interpolate_trajectories(sparse, 0, 10)&#10;    # Only the anchor should be present&#10;    assert len(interpolated) == 1&#10;    p = interpolated[0]&#10;    assert p.frame_idx == 4&#10;    assert p.source == &quot;homography&quot;&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>